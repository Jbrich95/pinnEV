% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/eGPD_NN.R
\name{eGPD.NN}
\alias{eGPD.NN}
\alias{eGPD.NN.train}
\alias{eGPD.NN.predict}
\title{eGPD PINN}
\usage{
eGPD.NN.train(Y.train, Y.valid = NULL, X.s, X.k, type = "MLP",
  offset = NULL, A = NULL, n.ep = 100, batch.size = 100,
  init.scale = NULL, init.kappa = NULL, init.xi = NULL, widths = c(6,
  3), filter.dim = c(3, 3), seed = NULL, init.wb_path = NULL,
  S_lambda = NULL)

eGPD.NN.predict(X.s, X.k, model, offset = NULL)
}
\arguments{
\item{Y.train, Y.valid}{a 2 or 3 dimensional array of training or validation real response values.
Missing values can be handled by setting corresponding entries to \code{Y.train} or \code{Y.valid} to \code{-1e10}.
The first dimension should be the observation indices, e.g., time.

If \code{type=="CNN"}, then \code{Y.train} and \code{Y.valid} must have three dimensions with the latter two corresponding to an \eqn{M} by \eqn{N} regular grid of spatial locations. If \code{type=="GCNN"}, then \code{Y.train} and \code{Y.valid} must have two dimensions with the latter corresponding to \eqn{M} spatial locations.
If \code{Y.valid==NULL}, no validation loss will be computed and the returned model will be that which minimises the training loss over \code{n.ep} epochs.}

\item{X.s}{list of arrays corresponding to complementary subsets of the \eqn{d\geq 1} predictors which are used for modelling the scale parameter \eqn{sigma}. Must contain at least one of the following three named entries:\describe{
\item{\code{X.lin.s}}{A 3 or 4 dimensional array of "linear" predictor values. One more dimension than \code{Y.train}. If \code{NULL}, a model without the linear component is built and trained.
The first 2/3 dimensions should be equal to that of \code{Y.train}; the last dimension corresponds to the chosen \eqn{l_1\geq 0} 'linear' predictor values.}
\item{\code{X.add.basis.s}}{A 4 or 5 dimensional array of basis function evaluations for the "additive" predictor values.
The first 2/3 dimensions should be equal to that of \code{Y.train}; the penultimate dimensions corresponds to the chosen \eqn{a_1\geq 0} 'linear' predictor values and the last dimension is equal to the number of knots used for estimating the splines. See example.
If \code{NULL}, a model without the additive component is built and trained.}
\item{\code{X.nn.s}}{A 3 or 4 dimensional array of "non-additive" predictor values.  If \code{NULL}, a model without the NN component is built and trained; if this is the case, then \code{type} has no effect.
The first 2/3 dimensions should be equal to that of \code{Y.train}; the last dimension corresponds to the chosen \eqn{d-l_1-a_1\geq 0} 'non-additive' predictor values.}
}
Note that \code{X.s} and \code{X.k} are the predictors for both \code{Y.train} and \code{Y.valid}. If \code{is.null(X.s)}, then \eqn{\sigma} will be treated as fixed over the predictors.}

\item{X.k}{similarly to \code{X.s}, but for modelling the shape parameter \eqn{\kappa>0}. Note that we require at least one of \code{!is.null(X.s)} or \code{!is.null(X.k)}, otherwise the formulated model will be fully stationary and will not be fitted.}

\item{type}{string defining the type of network to be built. If \code{type=="MLP"}, the network will have all densely connected layers; if \code{type=="CNN"},
the network will have all convolutional layers. If \code{type=="GCNN"}, then a graph convolutional neural network (with skip connections) is used and require \code{!is.null(A)}. Defaults to an MLP (currently the same network is used for all parameters, may change in future versions).}

\item{offset}{an array of strictly positive scalars the same dimension as \code{Y.train}, containing the offset values used in modelling the scale parameter. If \code{offset=NULL}, then no offset is used in the scale parameter (equivalently, \code{offset} is populated with ones). Defaults to \code{NULL}.}

\item{A}{\eqn{M \times M} adjacency matrix used if and only if \code{type=="GCNN"}. Must be supplied, defaults to \code{NULL}.}

\item{n.ep}{number of epochs used for training. Defaults to 1000.}

\item{batch.size}{batch size for stochastic gradient descent. If larger than \code{dim(Y.train)[1]}, i.e., the number of observations, then regular gradient descent used.}

\item{init.scale, init.kappa, init.xi}{sets the initial \eqn{\sigma,\kappa} and \eqn{\xi} estimates across all dimensions of \code{Y.train}. Overridden by \code{init.wb_path} if \code{!is.null(init.wb_path)}, but otherwise the initial parameters must be supplied.}

\item{widths}{vector of widths/filters for hidden layers. Number of layers is equal to \code{length(widths)}. Defaults to (6,3).}

\item{filter.dim}{if \code{type=="CNN"}, this 2-vector gives the dimensions of the convolution filter kernel; must have odd integer inputs. Note that filter.dim=c(1,1) is equivalent to \code{type=="MLP"}. The same filter is applied for each hidden layer across all parameters with NN predictors.}

\item{seed}{seed for random initial weights and biases.}

\item{init.wb_path}{filepath to a \code{keras} model which is then used as initial weights and biases for training the new model. The original model must have
the exact same architecture and trained with the same input data as the new model. If \code{NULL}, then initial weights and biases are random (with seed \code{seed}) but the
final layer has zero initial weights to ensure that the initial scale, kappa and shape estimates are \code{init.scale, init.kappa} and \code{init.xi}, respectively,  across all dimensions.}

\item{S_lambda}{list of smoothing penalty matrices for the splines modelling the effects of \code{X.add.basis.s} and \code{X.add.basis.k} on their respective parameters; each element only used if \code{!is.null(X.add.basis.s)} and \code{!is.null(X.add.basis.k)}, respectively. If \code{is.null(S_lambda[[1]])}, then no smoothing penalty used for \eqn{\sigma}; similarly for the second element and \eqn{\kappa}.}

\item{model}{fitted \code{keras} model. Output from \code{bGEVPP.NN.train}.}
}
\value{
\code{eGPD.NN.train} returns the fitted \code{model}.  \code{eGPD.NN.predict} is a wrapper for \code{keras::predict} that returns the predicted parameter estimates, and, if applicable, their corresponding linear regression coefficients and spline bases weights.
}
\description{
Build and train a partially-interpretable neural network for fitting an eGPD model
}
\details{
{
Consider a real-valued random variable \eqn{Y} and let \eqn{\mathbf{X}} denote a \eqn{d}-dimensional predictor set with observations \eqn{\mathbf{x}}.
For \eqn{i=1,2}, we define integers \eqn{l_i\geq 0,a_i \geq 0} and \eqn{0\leq l_i+a_i \leq d}, and let \eqn{\mathbf{X}^{(i)}_L, \mathbf{X}^{(i)}_A} and \eqn{\mathbf{X}^{(i)}_N} be distinct sub-vectors
of \eqn{\mathbf{X}}, with observations of each component denoted \eqn{\mathbf{x}^{(i)}_L, \mathbf{x}^{(i)}_A} and \eqn{\mathbf{x}^{(i)}_N}, respectively; the lengths of the sub-vectors are \eqn{l_i,a_i} and \eqn{d_i-l_i-a}, respectively.
We model \eqn{Y|\mathbf{X}=\mathbf{x}\sim\mbox{eGPD}(\sigma(\mathbf{x}),\kappa(\mathbf{x}),\xi)} for \eqn{\xi>0} with
\deqn{\sigma (\mathbf{x})=C(\mathbf{x})\exp\{\eta^{(1)}_0+m^{(1)}_L(\mathbf{x}^{(1)}_L)+m^{(1)}_A(x^{(1)}_A)+m^{(1)}_N(\mathbf{x}^{(1)}_N)\}} and
\deqn{\kappa (\mathbf{x})=\exp\{\eta^{(2)}_0+m^{(2)}_L(\mathbf{x}^{(2)}_L)+m^{(2)}_A(x^{(2)}_A)+m^{(2)}_N(\mathbf{x}^{(2)}_N)\}}
where \eqn{\eta^{(1)}_0,\eta^{(2)}_0} are constant intercepts and \eqn{C(\mathbf{x})>0} is a fixed offset term. The unknown functions \eqn{m^{(1)}_L,m^{(2)}_L} and
\eqn{m^{(1)}_A,m^{(2)}_A} are estimated using linear functions and splines, respectively, and are
both returned as outputs by \code{eGPD.NN.predict}; \eqn{m^{(1)}_N,m^{(2)}_N} are estimated using neural networks
(currently the same architecture is used for both parameters). The offset term is, by default, \eqn{C(\mathbf{x})=1} for all \eqn{\mathbf{x}}; if \code{!is.null(offset)}, then \code{offset} determines \eqn{C(\mathbf{x})} (see Cisneros et al., 2023). Note that \eqn{\xi>0} is fixed across all predictors; this may change in future versions.
Note that \eqn{\xi>0} is fixed across all predictors; this may change in future versions.

For details of the eGPD distribution, see \code{help(peGPD)}.

The model is fitted by minimising the negative log-likelihood associated with the bGEV model plus some smoothing penalty for the additive functions (determined by \code{S_lambda}; see Richards and Huser, 2022); training is performed over \code{n.ep} training epochs.
Although the model is trained by minimising the loss evaluated for \code{Y.train}, the final returned model may minimise some other loss.
The current state of the model is saved after each epoch, using \code{keras::callback_model_checkpoint}, if the value of some criterion subcedes that of the model from the previous checkpoint; this criterion is the loss evaluated for validation set \code{Y.valid} if \code{!is.null(Y.valid)} and for \code{Y.train}, otherwise.

A non-interpretable version of this model was exploited by Cisneros et al. (2024). Equivalence with their model is achieved by setting \code{X.k=NULL}, \code{X.lin.s=NULL}, \code{X.add.basis.s=NULL} and \code{type="GCNN"}. See \code{help(AusWild)}.

}
}
\examples{

set.seed(1)

# Create  predictors
preds<-rnorm(prod(c(2500,10,8)))


#Re-shape to a 3d array. First dimension corresponds to observations,
#last to the different components of the predictor set.
#Other dimensions correspond to indices of predictors, e.g., a grid of locations. Can be a 1D or 2D grid.
dim(preds)=c(2500,10,8)
#We have 2000 observations of eight predictors at 10 sites.


#Split predictors into linear, additive and nn. Different for kappa and scale parameters.
X.nn.k=preds[,,1:4] #Four nn predictors for kappa
X.lin.k=preds[,,5:6] #Two additive predictors for kappa
X.add.k=preds[,,7:8] #Two additive predictors for kappa

X.nn.s=preds[,,1:2] #Two nn predictors for sigma
X.lin.s=preds[,,3] #One linear predictor for sigma
dim(X.lin.s)=c(dim(X.lin.s),1) #Change dimension so consistent
X.add.s=preds[,,4] #One additive predictor for sigma
dim(X.add.s)=c(dim(X.add.s),1) #Change dimension so consistent

# Create toy response data

#Contribution to scale parameter
#Linear contribution
m_L_1 = 0.2*X.lin.s[,,1]

# Additive contribution
m_A_1 = 0.1*X.add.s[,,1]^2+0.2*X.add.s[,,1]

plot(X.add.s[,,1],m_A_1)

#Non-additive contribution - to be estimated by NN
m_N_1 = 0.2*exp(-4+X.nn.s[,,2]+X.nn.s[,,1])+
  0.1*sin(X.nn.s[,,1]-X.nn.s[,,2])*(X.nn.s[,,1]+X.nn.s[,,2])

sigma=0.4*exp(0.5+m_L_1+m_A_1+m_N_1+1) #Exponential link


#Contribution to kappa parameter
#Linear contribution
m_L_2 = 0.1*X.lin.k[,,1]-0.02*X.lin.k[,,2]

# Additive contribution
m_A_2 = 0.1*X.add.k[,,1]^2+0.1*X.add.k[,,1]-
  0.025*X.add.k[,,2]^3+0.025*X.add.k[,,2]^2

#Non-additive contribution - to be estimated by NN
m_N_2 = 0.5*exp(-3+X.nn.k[,,4]+X.nn.k[,,1])+
  sin(X.nn.k[,,1]-X.nn.k[,,2])*(X.nn.k[,,4]+X.nn.k[,,2])-
  cos(X.nn.k[,,4]-X.nn.k[,,1])*(X.nn.k[,,3]+X.nn.k[,,1])

kappa=exp(m_L_2+m_A_2+0.05 *m_N_2)  #Exponential link


xi=0.1 # Set xi

theta=array(dim=c(dim(sigma),3))
theta[,,1]=sigma; theta[,,2] = kappa; theta[,,3]=xi
#We simulate data from an eGPD model

#Simulate from eGPD model using same u as given above
Y=apply(theta,1:2,function(x) reGPD(1,sigma=x[1],kappa=x[2],xi=x[3]))



#Create training and validation, respectively.
#We mask 20\% of the Y values and use this for validation
#Masked values must be set to -1e10 and are treated as missing whilst training

mask_inds=sample(1:length(Y),size=length(Y)*0.8)

Y.train<-Y.valid<-Y #Create training and validation, respectively.
Y.train[-mask_inds]=-1e10
Y.valid[mask_inds]=-1e10



#To build a model with an additive component, we require an array of evaluations of
#the basis functions for each pre-specified knot and entry to X.add.k and X.add.s

rad=function(x,c){ #Define a basis function. Here we use the radial bases
  out=abs(x-c)^2*log(abs(x-c))
  out[(x-c)==0]=0
  return(out)
}

n.knot.s = 4; n.knot.k = 5# set number of knots.
#Must be the same for each additive predictor,
#but can differ between the parameters sigma and kappa


#Get knots for sigma predictor
knots.s=matrix(nrow=dim(X.add.s)[3],ncol=n.knot.s)
for( i in 1:dim(X.add.s)[3]){
  knots.s[i,]=quantile(X.add.s[,,i],probs=seq(0,1,length=n.knot.s))
}

#Evaluate radial basis functions for s_\beta predictor
X.add.basis.s<-array(dim=c(dim(X.add.s),n.knot.s))
for( i in 1:dim(X.add.s)[3]) {
  for(k in 1:n.knot.s) {
    X.add.basis.s[,,i,k]= rad(x=X.add.s[,,i],c=knots.s[i,k])
    #Evaluate rad at all entries to X.add.k and for all knots
  }}




#Create smoothing penalty matrix for the sigma additive function

# Set smoothness parameter
lambda = c(0.2)

S_lambda.s=matrix(0,nrow=n.knot.s*dim(X.add.s)[3],ncol=n.knot.s*dim(X.add.s)[3])
for(i in 1:dim(X.add.s)[3]){
  for(j in 1:n.knot.s){
    for(k in 1:n.knot.s){
      S_lambda.s[(j+(i-1)*n.knot.s),(k+(i-1)*n.knot.s)]=lambda[i]*rad(knots.s[i,j],knots.s[i,k])
    }
  }
}
#Get knots for kappa predictors
knots.k=matrix(nrow=dim(X.add.k)[3],ncol=n.knot.k)

#We set knots to be equally-spaced marginal quantiles
for( i in 1:dim(X.add.k)[3]){
  knots.k[i,]=quantile(X.add.k[,,i],probs=seq(0,1,length=n.knot.k))
}


#Evaluate radial basis functions for kappa predictors
X.add.basis.k<-array(dim=c(dim(X.add.k),n.knot.k))
for( i in 1:dim(X.add.k)[3]) {
  for(k in 1:n.knot.k) {
    X.add.basis.k[,,i,k]= rad(x=X.add.k[,,i],c=knots.k[i,k])
    #Evaluate rad at all entries to X.add.k and for all knots
  }}

#'#Create smoothing penalty matrix for the two kappa additive functions

# Set smoothness parameters for two functions
lambda = c(0.1,0.2)

S_lambda.k=matrix(0,nrow=n.knot.k*dim(X.add.k)[3],ncol=n.knot.k*dim(X.add.k)[3])
for(i in 1:dim(X.add.k)[3]){
  for(j in 1:n.knot.k){
    for(k in 1:n.knot.k){
      S_lambda.k[(j+(i-1)*n.knot.k),(k+(i-1)*n.knot.k)]=lambda[i]*rad(knots.k[i,j],knots.k[i,k])
    }
  }
}


#Join in one list
S_lambda =list("S_lambda.k"=S_lambda.k, "S_lambda.s"=S_lambda.s)

#lin+GAM+NN models defined for both scale and kappa parameters
X.s=list("X.nn.s"=X.nn.s, "X.lin.s"=X.lin.s,
         "X.add.basis.s"=X.add.basis.s) #Predictors for sigma
X.k=list("X.nn.k"=X.nn.k, "X.lin.k"=X.lin.k,
         "X.add.basis.k"=X.add.basis.k) #Predictors for kappa


#Fit the eGPD model. Note that training is not run to completion.
NN.fit<-eGPD.NN.train(Y.train, Y.valid,X.s,X.k, type="MLP",
                      n.ep=50, batch.size=50,init.scale=1, init.kappa=1,init.xi=0.1,
                      widths=c(6,3),seed=1,S_lambda=S_lambda)
out<-eGPD.NN.predict(X.s=X.s,X.k=X.k,NN.fit$model)

print("sigma linear coefficients: "); print(round(out$lin.coeff_s,2))
print("kappa linear coefficients: "); print(round(out$lin.coeff_k,2))

# Note that this is a simple example that can be run in a personal computer.


# #To save model, run
# NN.fit$model \%>\% save_model_tf("model_eGPD")
# #To load model, run
#  model  <- load_model_tf("model_eGPD",
#   custom_objects=list(
#     "eGPD_loss_S_lambda___S_lambda_"=
#       eGPD_loss(S_lambda=S_lambda))
#         )


# Plot splines for the additive predictors



#Sigma predictors
n.add.preds_s=dim(X.add.s)[length(dim(X.add.s))]
par(mfrow=c(1,n.add.preds_s))
for(i in 1:n.add.preds_s){
  plt.x=seq(from=min(knots.s[i,]),to=max(knots.s[i,]),length=1000)  #Create sequence for x-axis
  
  tmp=matrix(nrow=length(plt.x),ncol=n.knot.s)
  for(j in 1:n.knot.s){
    tmp[,j]=rad(plt.x,knots.s[i,j]) #Evaluate radial basis function of plt.x and all knots
  }
  plt.y=tmp\%*\%out$gam.weights_s[i,]
  plot(plt.x,plt.y,type="l",main=paste0("sigma spline: predictor ",i),xlab="x",ylab="f(x)")
  points(knots.s[i,],rep(mean(plt.y),n.knot.s),col="red",pch=2)
  #Adds red triangles that denote knot locations
  
}

#Kappa predictors
n.add.preds_k=dim(X.add.k)[length(dim(X.add.k))]
par(mfrow=c(1,n.add.preds_k))
for(i in 1:n.add.preds_k){
  plt.x=seq(from=min(knots.k[i,]),to=max(knots.k[i,]),length=1000)  #Create sequence for x-axis
  
  tmp=matrix(nrow=length(plt.x),ncol=n.knot.k)
  for(j in 1:n.knot.k){
    tmp[,j]=rad(plt.x,knots.k[i,j]) #Evaluate radial basis function of plt.x and all knots
  }
  plt.y=tmp\%*\%out$gam.weights_k[i,]
  plot(plt.x,plt.y,type="l",main=paste0("kappa spline: predictor ",i),xlab="x",ylab="f(x)")
  points(knots.k[i,],rep(mean(plt.y),n.knot.k),col="red",pch=2)
  #Adds red triangles that denote knot locations
  
}
}
\references{
{
Papastathopoulos, I. and Tawn, J. A. (2013), \emph{xtended generalised Pareto models for tail estimation}, Journal of Statistical Planning and Inference, 43(1):131–1439.
(\href{https://doi.org/10.1016/j.jspi.2012.07.001}{doi})

Naveau, P., Huser, R., Ribereau, P., and Hannart, A. (2016), \emph{Modeling jointly low, moderate, and heavy rainfall intensities without a threshold selection}, Water Resources Research, 2(4):2753–2769.
(\href{https://doi.org/10.1002/2015WR018552}{doi})

Richards, J. and Huser, R. (2024+), \emph{Regression modelling of spatiotemporal extreme U.S. wildfires via partially-interpretable neural networks}. (\href{https://arxiv.org/abs/2208.07581}{arXiv:2208.07581}).

Cisneros, D., Richards, J., Dahal, A., Lombardo, L., and Huser, R. (2024), \emph{Deep learning-based graphical regression for jointly moderate and extreme Australian wildfires.} Spatial Statistics, 53:100811. (\href{https://doi.org/10.1016/j.spasta.2024.100811}{doi}).
}
}
